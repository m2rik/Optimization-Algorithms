{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('house.csv')\n",
    "df.drop('Unnamed: 0', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rit\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   1, 1370],\n",
       "       [   4, 2060],\n",
       "       [   4, 1738],\n",
       "       ...,\n",
       "       [   3, 1498],\n",
       "       [   3, 1695],\n",
       "       [   2, 1384]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.as_matrix(columns=['bd','sqft'])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rit\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "def feature_normalize(X):\n",
    "    n_features = X.shape[1]\n",
    "    means = np.array([np.mean(X[:,i]) for i in range(n_features)])\n",
    "    stddevs = np.array([np.std(X[:,i]) for i in range(n_features)])\n",
    "    normalized = (X - means) / stddevs\n",
    "\n",
    "    return normalized\n",
    "\n",
    "X = df.as_matrix(columns=['bd','sqft'])\n",
    "X = feature_normalize(X)\n",
    "X = np.column_stack((np.ones(len(X)), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['price'].values / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "## Compute cost and theta for Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    return np.sum(np.square(np.matmul(X, theta) - y)) / (2 * len(y))\n",
    "\n",
    "def gradient_descent_multi(X, y, theta, alpha, iterations,eps):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    m = len(X)\n",
    "    j_history = np.zeros(iterations)\n",
    "    theta_1_hist = [] \n",
    "    theta_2_hist = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "            gradient = (1/m) * np.matmul(X.T, np.matmul(X, theta) - y)\n",
    "            theta = theta - alpha * gradient\n",
    "            eps = np.linalg.norm(gradient)\n",
    "            if eps < 0.01:\n",
    "                break\n",
    "            else:\n",
    "                j_history[i] = compute_cost(X,y,theta)\n",
    "                theta_1_hist.append(theta[1])\n",
    "                theta_2_hist.append(theta[2])\n",
    "    return theta ,j_history, theta_1_hist, theta_2_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the GD with the actual solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thetas [659.36585116 105.97741737  59.07633606] Cost [1255.06624625]\n"
     ]
    }
   ],
   "source": [
    "#for alpha 0.1\n",
    "theta = np.zeros(2)\n",
    "alpha = 0.1\n",
    "iterations = 100\n",
    "eps = 1\n",
    "#Computing the gradient descent\n",
    "theta_result,J_history, theta_0, theta_1 = gradient_descent_multi(X,y,theta,alpha,iterations,eps)\n",
    "print(\"Thetas\",theta_result,'Cost',J_history[99:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### θ=(XTX)−1XTy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[659.3833653  111.79044715  53.26330653]\n",
      "1249.6484990834601\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "\n",
    "theta_norm = []\n",
    "def normal_eq(X, y):\n",
    "    theta_norm.append(inv(X.T.dot(X)).dot(X.T).dot(y))\n",
    "    return inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "theta_n = normal_eq(X, y)\n",
    "cost = compute_cost(X, y, theta_n)\n",
    "print(theta_n)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that there is not much difference in the parameters of the solution and that there is convergence between both the solutions. Next we will try to iterate the solution for different Values of Alpha(Learning Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "## CHECK OTHER FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thetas [659.36585116 105.97741737  59.07633606] Cost [1255.06624625]\n"
     ]
    }
   ],
   "source": [
    "#for alpha 0.1\n",
    "theta = np.zeros(2)\n",
    "alpha = 0.1\n",
    "iterations = 100\n",
    "eps = 1\n",
    "#Computing the gradient descent\n",
    "theta_result,J_history, theta_0, theta_1 = gradient_descent_multi(X,y,theta,alpha,iterations,eps)\n",
    "print(\"Thetas\",theta_result,'Cost',J_history[99:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thetas [-5.76893113e+110 -9.19192803e+125 -9.19192803e+125] Cost [1.55436972e+252]\n"
     ]
    }
   ],
   "source": [
    "#For alpha 10\n",
    "theta = np.random.randn(3, 1)\n",
    "alpha = 10\n",
    "iterations = 100\n",
    "theta_result,J_history, theta_0, theta_1 = gradient_descent_multi(X,y,theta,alpha,iterations,eps)\n",
    "print(\"Thetas\",theta_result,'Cost',J_history[99:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diverging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rit\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "C:\\Users\\Rit\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in square\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thetas [-3.47811935e+213 -1.42557420e+228 -1.42557420e+228] Cost [inf]\n"
     ]
    }
   ],
   "source": [
    "#For alpha 100\n",
    "theta = np.random.randn(3, 1)\n",
    "alpha = 100\n",
    "iterations = 100\n",
    "theta_result,J_history, theta_0, theta_1 = gradient_descent_multi(X,y,theta,alpha,iterations,eps)\n",
    "print(\"Thetas\",theta_result,'Cost',J_history[99:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diverging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that for values of alpha within the range of 0-1 there is a high chance of convergence and over the value of 5 it starts diverging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Alpha = Alpha/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thetas [659.3833653   93.46559312  78.57124672] Cost [1348.38327766]\n",
      "Thetas [8985.06411115 4779.14373065 4715.90325319] Cost [74695092.74802636]\n",
      "Thetas [-2.22601660e+07  1.03325363e+08  1.03322412e+08] Cost [1.988779e+16]\n"
     ]
    }
   ],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    return np.sum(np.square(np.matmul(X, theta) - y)) / (2 * len(y))\n",
    "\n",
    "def gradient_descent_multi(X, y, theta, alpha, iterations,eps):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    m = len(X)\n",
    "    j_history = np.zeros(iterations)\n",
    "    theta_1_hist = [] \n",
    "    theta_2_hist = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "            alpha = alpha/(i+1)\n",
    "            gradient = (1/m) * np.matmul(X.T, np.matmul(X, theta) - y)\n",
    "            theta = theta - alpha * gradient\n",
    "            eps = np.linalg.norm(gradient)\n",
    "            if eps < 0.01:\n",
    "                break\n",
    "            else:\n",
    "                j_history[i] = compute_cost(X,y,theta)\n",
    "                theta_1_hist.append(theta[1])\n",
    "                theta_2_hist.append(theta[2])\n",
    "    return theta ,j_history, theta_1_hist, theta_2_hist\n",
    "\n",
    "theta = np.random.randn(3, 1)\n",
    "alpha = [1,10,100]\n",
    "iterations = 100\n",
    "\n",
    "for i in alpha:\n",
    "    theta_result,J_history, theta_0, theta_1 = gradient_descent_multi(X,y,theta,i,iterations,eps)\n",
    "    print(\"Thetas\",theta_result,'Cost',J_history[99:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Alpha  = Alpha/Sqrt(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thetas [659.3833653   90.15334269  70.07308384] Cost [1319.61265055]\n",
      "Thetas [-4376.93517548 42618.53654796 42560.8597449 ] Cost [3.33671788e+09]\n",
      "Thetas [1.23605731e+10 3.92250293e+10 3.92250146e+10] Cost [2.90692007e+21]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def compute_cost(X, y, theta):\n",
    "    return np.sum(np.square(np.matmul(X, theta) - y)) / (2 * len(y))\n",
    "\n",
    "def gradient_descent_multi(X, y, theta, alpha, iterations,eps):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    m = len(X)\n",
    "    j_history = np.zeros(iterations)\n",
    "    theta_1_hist = [] \n",
    "    theta_2_hist = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "            alpha = alpha/math.sqrt(i+1)\n",
    "            gradient = (1/m) * np.matmul(X.T, np.matmul(X, theta) - y)\n",
    "            theta = theta - alpha * gradient\n",
    "            eps = np.linalg.norm(gradient)\n",
    "            if eps < 0.01:\n",
    "                break\n",
    "            else:\n",
    "                j_history[i] = compute_cost(X,y,theta)\n",
    "                theta_1_hist.append(theta[1])\n",
    "                theta_2_hist.append(theta[2])\n",
    "    return theta ,j_history, theta_1_hist, theta_2_hist\n",
    "\n",
    "theta = np.random.randn(3, 1)\n",
    "alpha = [1,10,100]\n",
    "iterations = 100\n",
    "\n",
    "for i in alpha:\n",
    "    theta_result,J_history, theta_0, theta_1 = gradient_descent_multi(X,y,theta,i,iterations,eps)\n",
    "    print(\"Thetas\",theta_result,'Cost',J_history[99:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta:  [[659.        ]\n",
      " [ 86.37892068]\n",
      " [ 72.70448815]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def cal_cost(X,y,theta):\n",
    "#     m = len(y)\n",
    "#     predictions = X.dot(theta)\n",
    "#     cost = (1/2*m) * np.sum(np.square(predictions-y))\n",
    "    cost =  np.sum(np.square(np.matmul(X, theta) - y)) / (2 * len(y))\n",
    "    return cost\n",
    "\n",
    "def sto_gradient_descent(X, y, theta, alpha, iterations):\n",
    "    \n",
    "        #new\n",
    "    r_list=random.sample(list(y), 5000)\n",
    "    m=len(r_list)\n",
    "\n",
    "    J_history_2  = []\n",
    "    theta_1_hist = []\n",
    "    theta_2_hist = []\n",
    "    cost =0.0\n",
    "    eps = 1\n",
    "    i = 0\n",
    "    while eps > 0.1:\n",
    "        rand_ind = np.random.randint(0,m)\n",
    "        X_i = X[rand_ind,:].reshape(1,X.shape[1])\n",
    "        y_i = y[rand_ind].reshape(1,1)\n",
    "        prediction = X_i.T.dot((np.dot(X_i,theta) - y_i))\n",
    "\n",
    "        theta = theta -(1/m) * alpha *(prediction ) \n",
    "        theta[0] = 659\n",
    "        cost = cal_cost(theta,X_i,y_i)\n",
    "        \n",
    "        J_history_2.append(cost)\n",
    "        theta_1_hist.append(theta[1])\n",
    "        theta_2_hist.append(theta[2])\n",
    "        eps = np.linalg.norm(prediction,ord=2)\n",
    "        i+=1\n",
    "    print(\"theta: \", theta)\n",
    "    return theta,theta_1_hist,theta_2_hist, J_history_2\n",
    "        \n",
    "        \n",
    "theta = np.random.randn(3, 1)\n",
    "alpha = 20\n",
    "iterations = 1\n",
    "\n",
    "theta_result,theta_0, theta_1, J_history= sto_gradient_descent(X,y,theta,alpha,iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For different values of Alpha in Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta:  [[659.        ]\n",
      " [ 20.30542646]\n",
      " [ 17.81269361]]\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.randn(3, 1)\n",
    "alpha = 0.1\n",
    "iterations = 1\n",
    "\n",
    "theta_result,theta_0, theta_1, J_history= sto_gradient_descent(X,y,theta,alpha,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta:  [[659.        ]\n",
      " [ 98.93747996]\n",
      " [ 70.03865768]]\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.randn(3, 1)\n",
    "alpha = 20\n",
    "iterations = 1\n",
    "\n",
    "theta_result,theta_0, theta_1, J_history= sto_gradient_descent(X,y,theta,alpha,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta:  [[659.        ]\n",
      " [106.78103757]\n",
      " [ 60.56160322]]\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.randn(3, 1)\n",
    "alpha = 100\n",
    "iterations = 1\n",
    "\n",
    "theta_result,theta_0, theta_1, J_history= sto_gradient_descent(X,y,theta,alpha,iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yes. SGD is faster than Gradeint Descent because of the batch updates and random initialization of thetas. The approximation is coming from one data point (or several data points called mini batch). Therefore, in SGD, we can update the parameters very quickly. In addition, if we \"loop\" over all data - 1 Epoch, we actually have 1 million updates. The main reason to use SGD is to be able to reduce the computation cost of gradient while still maintaining the gradient direction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
